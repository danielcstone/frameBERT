{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Korean FrameNet ###\n",
      "\t# contact: hahmyg@kaist, hahmyg@gmail.com #\n",
      "\n",
      "\n",
      "###DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import frame_parser\n",
    "from src import dataio\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import random\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if device != \"cpu\":\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행시간 측정 함수\n",
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60)\n",
    "    \n",
    "    result = '{}hour:{}min:{}sec'.format(t_hour,t_min,t_sec)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "with open('./data/frame_coreFE_list.json','r') as f:\n",
    "    frame_coreFE = json.load(f)\n",
    "\n",
    "def weighting(gold_frame, pred_frame, gold_args, pred_args):\n",
    "    \n",
    "    weighted_gold_frame, weighted_pred_frame = ['B-'+gold_frame], ['B-'+pred_frame]\n",
    "    weighted_gold_args, weighted_pred_args = gold_args.copy(), pred_args.copy()\n",
    "      \n",
    "    weighted_gold_frame.append('B-'+gold_frame)\n",
    "    weighted_pred_frame.append('B-'+pred_frame)        \n",
    "\n",
    "    for i in range(len(gold_args)):\n",
    "        gold_arg = gold_args[i]\n",
    "        pred_arg = pred_args[i]\n",
    "\n",
    "        fe = gold_arg.split('-')[-1]\n",
    "        if fe in frame_coreFE[gold_frame]:\n",
    "            weighted_gold_args.append(gold_arg)\n",
    "            weighted_pred_args.append(pred_arg)\n",
    "        elif fe == 'ARG':\n",
    "            weighted_gold_args.append(gold_arg)\n",
    "            weighted_pred_args.append(pred_arg)\n",
    "        else:\n",
    "            weighted_gold_args.append('O')\n",
    "            weighted_pred_args.append('O')\n",
    "        \n",
    "    return weighted_gold_frame, weighted_pred_frame, weighted_gold_args, weighted_pred_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(srl=False, masking=False, viterbi=False, language=False, model_path=False, \n",
    "         result_dir=False, train_lang=False, tgt=False,\n",
    "         pretrained=\"bert-base-multilingual-cased\"):\n",
    "    if not result_dir:\n",
    "        result_dir = '/disk/data/models/'+model_dir.split('/')[-2]+'-result/'\n",
    "    else:\n",
    "        pass\n",
    "    if result_dir[-1] != '/':\n",
    "        result_dir = result_dir+'/'\n",
    "        \n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "        \n",
    "    if not train_lang:\n",
    "        train_lang = language\n",
    "    \n",
    "    fname = result_dir+train_lang+'_for_'+language\n",
    "        \n",
    "    if masking:\n",
    "        fname = fname + '_with_masking_result.txt'\n",
    "    else:\n",
    "        fname = fname +'_result.txt'\n",
    "        \n",
    "    print('### Your result would be saved to:', fname)\n",
    "        \n",
    "    trn, dev, tst = dataio.load_data(srl=srl, language=language, exem=False, info=False)\n",
    "    if srl == 'framenet-argid':\n",
    "        trn = dataio.fe2arg(trn)\n",
    "        dev = dataio.fe2arg(dev)\n",
    "        tst = dataio.fe2arg(tst)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('### EVALUATION')\n",
    "    print('MODE:', srl)\n",
    "    print('target LANGUAGE:', language)\n",
    "    print('trained LANGUAGE:', train_lang)\n",
    "    print('Viterbi:', viterbi)\n",
    "    print('masking:', masking)\n",
    "    print('using TGT token:', tgt)\n",
    "    tic()    \n",
    "    \n",
    "    models = glob.glob(model_path+'*/')\n",
    "    \n",
    "    eval_result = []\n",
    "    for m in models:        \n",
    "        print('### model dir:', m)\n",
    "        print('### TARGET LANGUAGE:', language)\n",
    "        torch.cuda.set_device(device)\n",
    "        \n",
    "        \n",
    "        model = frame_parser.FrameParser(srl=srl,gold_pred=True, model_path=m, viterbi=viterbi, \n",
    "                                         masking=masking, language=language, tgt=tgt,\n",
    "                                         pretrained=pretrained)\n",
    "\n",
    "        gold_senses, pred_senses, gold_args, pred_args = [],[],[],[]        \n",
    "        gold_full_all, pred_full_all = [],[]\n",
    "\n",
    "        for instance in tst:\n",
    "            torch.cuda.set_device(device)\n",
    "            result = model.parser(instance)\n",
    "\n",
    "            gold_sense = [i for i in instance[2] if i != '_'][0]\n",
    "            pred_sense = [i for i in result[0][2] if i != '_'][0]\n",
    "\n",
    "\n",
    "            gold_arg = [i for i in instance[3] if i != 'X']\n",
    "            pred_arg = [i for i in result[0][3]]\n",
    "\n",
    "            gold_senses.append(gold_sense)\n",
    "            pred_senses.append(pred_sense)\n",
    "            \n",
    "            weighted_gold_frame, weighted_pred_frame, weighted_gold_arg, weighted_pred_arg = weighting(gold_sense, pred_sense, gold_arg, pred_arg)\n",
    "            \n",
    "            gold_args.append(weighted_gold_arg)\n",
    "            pred_args.append(weighted_pred_arg)\n",
    "\n",
    "#             if 'framenet' in srl:\n",
    "            gold_full = []\n",
    "            gold_full += weighted_gold_frame\n",
    "            gold_full += weighted_gold_arg\n",
    "\n",
    "            pred_full = []\n",
    "            pred_full += weighted_pred_frame\n",
    "            pred_full += weighted_pred_arg\n",
    "\n",
    "            gold_full_all.append(gold_full)\n",
    "            pred_full_all.append(pred_full)\n",
    "                \n",
    "#             break\n",
    "                \n",
    "        del model\n",
    "            \n",
    "        acc = accuracy_score(gold_senses, pred_senses)\n",
    "        arg_f1 = f1_score(gold_args, pred_args)\n",
    "        arg_precision = precision_score(gold_args, pred_args)\n",
    "        arg_recall = recall_score(gold_args, pred_args)\n",
    "\n",
    "        epoch = m.split('/')[-2]\n",
    "        print('# EPOCH:', epoch)\n",
    "        print(\"SenseId Accuracy: {}\".format(acc))\n",
    "        print(\"ArgId Precision: {}\".format(arg_precision))\n",
    "        print(\"ArgId Recall: {}\".format(arg_recall))\n",
    "        print(\"ArgId F1: {}\".format(arg_f1))\n",
    "#         if 'framenet' in srl:\n",
    "        full_f1 = f1_score(gold_full_all, pred_full_all)\n",
    "        full_precision = precision_score(gold_full_all, pred_full_all)\n",
    "        full_recall = recall_score(gold_full_all, pred_full_all)\n",
    "        print(\"full-structure Precision: {}\".format(full_precision))\n",
    "        print(\"full-structure Recall: {}\".format(full_recall))\n",
    "        print(\"full-structure F1: {}\".format(full_f1))\n",
    "        print('-----processing time:', tac())\n",
    "        print('')\n",
    "\n",
    "        model_result = []\n",
    "        model_result.append(epoch)\n",
    "        model_result.append(acc)\n",
    "        model_result.append(arg_precision)\n",
    "        model_result.append(arg_recall)\n",
    "        model_result.append(arg_f1)\n",
    "#         if 'framenet' in srl:\n",
    "        model_result.append(full_precision)\n",
    "        model_result.append(full_recall)\n",
    "        model_result.append(full_f1)\n",
    "        model_result = [str(i) for i in model_result]\n",
    "        eval_result.append(model_result)\n",
    "        \n",
    "#         break\n",
    "    \n",
    "    with open(fname,'w') as f:\n",
    "        f.write('epoch'+'\\t''SenseID'+'\\t'+'Arg_P'+'\\t'+'Arg_R'+'\\t'+'ArgF1'+'\\t'+'full_P'+'\\t'+'full_R'+'\\t'+'full_F1'+'\\n')\n",
    "        for i in eval_result:\n",
    "            line = '\\t'.join(i)\n",
    "            f.write(line+'\\n')\n",
    "            \n",
    "        print('\\n\\t### Your result is saved at:', fname)\n",
    "        print('...done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl = 'framenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평가 1-1. ArgID 모델 평가 for En\n",
    "\n",
    "# language = 'en'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for ARGID Model (masking) for English')\n",
    "# model_path = '/disk/frameBERT/models/en_argid/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl='framenet-argid', language=language, masking=True, viterbi=False, tgt=True, train_lang='en_argid', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 1-2. ArgID_only 모델 평가 for En\n",
    "\n",
    "# language = 'en'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for ARGID_only Model (masking) for English')\n",
    "# model_path = '/disk/frameBERT/models/en_argid_only_arg/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl='framenet-argid', language=language, masking=True, viterbi=False, tgt=True, train_lang='en_argid_only_arg', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 1-3. ArgID 모델 평가 for Ko\n",
    "\n",
    "language = 'ko'\n",
    "\n",
    "print('\\n####################################################')\n",
    "print('\\t###eval for ARGID Model (masking) for Korean')\n",
    "model_path = '/disk/frameBERT/models/ko_argid/'\n",
    "result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "test(srl='framenet-argid', language=language, masking=True, viterbi=False, tgt=True, train_lang='ko_argid', \n",
    "     model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 1-4. ArgID_only 모델 평가 for Ko\n",
    "\n",
    "language = 'ko'\n",
    "\n",
    "print('\\n####################################################')\n",
    "print('\\t###eval for ARGID_only Model (masking) for Korean')\n",
    "model_path = '/disk/frameBERT/models/ko_argid_only_arg/'\n",
    "result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "test(srl='framenet-argid', language=language, masking=True, viterbi=False, tgt=True, train_lang='ko_argid_only_arg', \n",
    "     model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 2. En 모델을 En에 평가\n",
    "\n",
    "# language = 'en'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for en Model (masking) for English')\n",
    "# model_path = '/disk/frameBERT/models/enModel-with-exemplar/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='en', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 3. Ko 모델을 Ko에 평가\n",
    "\n",
    "# language = 'ko'\n",
    "\n",
    "# print('\\t###eval for ko Model (masking) for Korean')\n",
    "# model_path = '/disk/frameBERT/models/koModel/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='ko', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 4. Cross-lingual 모델을 Ko에 평가\n",
    "\n",
    "# language = 'ko'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for Cross-lingual Model (masking) for Korean')\n",
    "# model_path = '/disk/frameBERT/models/crosslingual/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='cross', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 5. En 모델을 Ko에 평가 (Zero-shot)\n",
    "\n",
    "# language = 'ko'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for En Model (masking) for Korean')\n",
    "# model_path = '/disk/frameBERT/models/enModel-with-exemplar/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='zero-shot', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 평가 6. Joint 모델을 Ko에 평가\n",
    "\n",
    "# language = 'ko'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for Joint Model (masking) for Korean')\n",
    "# model_path = '/disk/frameBERT/models/joint/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='multilingual-100', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 7. Joint 모델을 En에 평가\n",
    "\n",
    "# language = 'en'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for Joint Model (masking) for English')\n",
    "# model_path = '/disk/frameBERT/models/joint/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='multilingual-100', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 8. Joint 모델 - 10%, 25%, 50%, 75%\n",
    "\n",
    "# language = 'ko'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for Joint Model (masking) for Korean by increasing dataset')\n",
    "# model_path = '/disk/data/models/increasing_data/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='multilingual-increasing', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 9. ko 모델 - 10%, 25%, 50%, 75%\n",
    "\n",
    "# language = 'ko'\n",
    "\n",
    "# print('\\n####################################################')\n",
    "# print('\\t###eval for 10% Joint Model (masking) for Korean by increasing dataset')\n",
    "# model_path = '/disk/data/models/increasing_data_only_ko/'\n",
    "# result_dir = '/disk/frameBERT/eval_result/'\n",
    "\n",
    "# test(srl=srl, language=language, masking=True, viterbi=False, tgt=True, train_lang='ko-increasing', \n",
    "#      model_path=model_path, result_dir=result_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
