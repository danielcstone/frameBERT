{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from transformers import *\n",
    "from frameBERT.src import utils\n",
    "from frameBERT.src import dataio\n",
    "from frameBERT.src import eval_fn\n",
    "from frameBERT import frame_parser\n",
    "from frameBERT.src.modeling import BertForJointShallowSemanticParsing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if device != \"cpu\":\n",
    "    torch.cuda.set_device(0)\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(0)   \n",
    "random.seed(0)\n",
    "\n",
    "from torch import autograd\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'\n",
    "    \n",
    "# 실행시간 측정 함수\n",
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60)\n",
    "    \n",
    "    result = '{}hour:{}min:{}sec'.format(t_hour,t_min,t_sec)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "\ttask: framenet\n",
      "\tlanguage: multilingual\n",
      "\tfn_version: 1.2\n",
      "used dictionary:\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_lu2idx.json\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_lufrmap.json\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n"
     ]
    }
   ],
   "source": [
    "srl = 'framenet'\n",
    "language = 'multilingual'\n",
    "fnversion = '1.2'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train', required=True, help='choose the training data')\n",
    "parser.add_argument('--model_path', required=True, help='model directory', default='/disk/frameBERT/cltl_eval/models/ekfn')\n",
    "parser.add_argument('--pretrained_model', required=False, help='bert-base-multilingual-cased')\n",
    "parser.add_argument('--early_stopping', required=False, help='early_stopping', default=True)\n",
    "parser.add_argument('--epochs', required=False, help='early_stopping', default=20)> 1202\n",
    "args = parser.parse_args()\n",
    "\n",
    "if '/models' not in args.model_path:\n",
    "    args.model_path = '/disk/frameBERT/cltl_eval/models/'+args.model_path\n",
    "\n",
    "# if args.model_path[-1] != '/':\n",
    "#     args.model_path = args.model_path+'/'\n",
    "\n",
    "print('#####')\n",
    "print('\\ttraining data:', args.train)\n",
    "print('\\tpretrained_model:', args.pretrained_model)\n",
    "print('\\tearly_stopping:', args.early_stopping)\n",
    "print('\\tepochs:', args.epochs)\n",
    "\n",
    "\n",
    "bert_io = utils.for_BERT(mode='train', language=language, masking=True, fnversion=fnversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of instances in trn: 19391\n",
      "# of instances in dev: 2272\n",
      "# of instances in tst: 6714\n",
      "data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]\n",
      "\n",
      "### loading Korean FrameNet (from efn )\n",
      "tuples: (trn, tst)\n",
      "10647 3550\n",
      "\n",
      "### loading Korean FrameNet (from jfn )\n",
      "tuples: (trn, tst)\n",
      "2200 1000\n",
      "\n",
      "### loading Korean FrameNet (from sejong )\n",
      "tuples: (trn, unlabel_data, tst)\n",
      "500 4212 1000\n",
      "\n",
      "### loading Korean FrameNet (from propbank )\n",
      "tuples: (trn, unlabel_data, tst)\n",
      "500 852 1000\n"
     ]
    }
   ],
   "source": [
    "from koreanframenet import koreanframenet\n",
    "kfn = koreanframenet.interface(version=fnversion)\n",
    "\n",
    "en_trn, en_dev, en_tst = dataio.load_data(srl=srl, language='en')\n",
    "\n",
    "ekfn_trn_d, ekfn_tst_d = kfn.load_data(source='efn')\n",
    "jkfn_trn_d, jkfn_tst_d = kfn.load_data(source='jfn')\n",
    "skfn_trn_d, skfn_unlabel_d, skfn_tst_d = kfn.load_data(source='sejong')\n",
    "pkfn_trn_d, pkfn_unlabel_d, pkfn_tst_d = kfn.load_data(source='propbank')\n",
    "\n",
    "ekfn_trn = dataio.data2tgt_data(ekfn_trn_d, mode='train')\n",
    "ekfn_tst = dataio.data2tgt_data(ekfn_tst_d, mode='train')\n",
    "\n",
    "jkfn_trn = dataio.data2tgt_data(jkfn_trn_d, mode='train')\n",
    "jkfn_tst = dataio.data2tgt_data(jkfn_tst_d, mode='train')\n",
    "\n",
    "skfn_trn = dataio.data2tgt_data(skfn_trn_d, mode='train')\n",
    "skfn_unlabel = dataio.data2tgt_data(skfn_unlabel_d, mode='train')\n",
    "skfn_tst = dataio.data2tgt_data(skfn_tst_d, mode='train')\n",
    "\n",
    "pkfn_trn = dataio.data2tgt_data(pkfn_trn_d, mode='train')\n",
    "pkfn_unlabel = dataio.data2tgt_data(pkfn_unlabel_d, mode='train')\n",
    "pkfn_tst = dataio.data2tgt_data(pkfn_tst_d, mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: en_trn + ekfn_trn + jkfn\n",
      "# of instance in training data: 32238\n",
      "training data example:\n",
      "[['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]\n",
      "[['각지에', '「뇌', '외상', '친우회」', '및', '관련', '단체가', '결성되어', '가는', '가운데,', '가족', '단체의', '권고로', '장애', '연금,', '장애인', '수첩의', '취득률도', '오르고', '있어,', '작년의', '해당', '단체의', '조사에', '따르면,', '장애인', '수첩을', '가지고', '있지', '않은', '사람은', '약', '20%로', '<tgt>', '나타났다.', '</tgt>'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '나타나다.v', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Change_position_on_a_scale', '_'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Item', 'I-Item', 'I-Item', 'I-Item', 'I-Item', 'I-Item', 'B-Final_value', 'I-Final_value', 'X', 'O', 'X']]\n"
     ]
    }
   ],
   "source": [
    "trn_data = {}\n",
    "trn_data['ekfn'] = ekfn_trn\n",
    "trn_data['jkfn'] = jkfn_trn\n",
    "trn_data['skfn'] = skfn_trn\n",
    "trn_data['pkfn'] = pkfn_trn\n",
    "trn_data['all'] = ekfn_trn + jkfn_trn + skfn_trn + pkfn_trn + skfn_unlabel + pkfn_unlabel\n",
    "\n",
    "tst_data = {}\n",
    "tst_data['ekfn'] = ekfn_tst\n",
    "tst_data['jkfn'] = jkfn_tst\n",
    "tst_data['skfn'] = skfn_tst\n",
    "tst_data['pkfn'] = pkfn_tst\n",
    "tst_data['all'] = ekfn_tst + jkfn_tst + skfn_tst + pkfn_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(PRETRAINED_MODEL=\"bert-base-multilingual-cased\",\n",
    "          model_dir=False, epochs=20, fnversion=False, early_stopping=True, batch_size=6, \n",
    "          trn=False, dev=False):\n",
    "    \n",
    "    tic()\n",
    "    \n",
    "    if model_dir[-1] != '/':\n",
    "        model_dir = model_dir+'/'\n",
    "        \n",
    "    if early_stopping == True:\n",
    "        model_saved_path = model_dir+'best/'\n",
    "        model_dummy_path = model_dir+'dummy/'\n",
    "        if not os.path.exists(model_dummy_path):\n",
    "            os.makedirs(model_dummy_path)\n",
    "    else:\n",
    "        model_saved_path = model_dir        \n",
    "            \n",
    "    if not os.path.exists(model_saved_path):\n",
    "        os.makedirs(model_saved_path)\n",
    "    print('\\nyour model would be saved at', model_saved_path)\n",
    "\n",
    "    # load a pre-trained model first\n",
    "    print('\\nloading a pre-trained model...')\n",
    "    model = BertForJointShallowSemanticParsing.from_pretrained(PRETRAINED_MODEL, \n",
    "                                                               num_senses = len(bert_io.sense2idx), \n",
    "                                                               num_args = len(bert_io.bio_arg2idx),\n",
    "                                                               lufrmap=bert_io.lufrmap, \n",
    "                                                               frargmap = bert_io.bio_frargmap)\n",
    "    model.to(device)\n",
    "    print('... is done.', tac())\n",
    "    \n",
    "    print('\\nconverting data to BERT input...')\n",
    "    print('# of instances:', len(trn))\n",
    "    trn_data = bert_io.convert_to_bert_input_JointShallowSemanticParsing(trn)\n",
    "    sampler = RandomSampler(trn)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    print('... is done', tac())\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    \n",
    "    best_score = 0\n",
    "    renew_stack = 0\n",
    "    \n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        \n",
    "        # TRAIN loop\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            model.train()\n",
    "            # add batch to gpu\n",
    "            torch.cuda.set_device(device)\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_lus, b_input_senses, b_input_args, b_token_type_ids, b_input_masks = batch            \n",
    "            loss = model(b_input_ids, lus=b_input_lus, senses=b_input_senses, args=b_input_args,\n",
    "                     token_type_ids=b_token_type_ids, attention_mask=b_input_masks)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "#             break\n",
    "\n",
    "        if early_stopping == True:\n",
    "            model.save_pretrained(model_dummy_path)\n",
    "            \n",
    "            # evaluate the model using dev dataset\n",
    "            print('\\n### eval by dev')\n",
    "            test_model = frame_parser.FrameParser(srl=srl, gold_pred=True, \n",
    "                                                  info=False, model_path=model_dummy_path, language=language)\n",
    "            parsed_result = []\n",
    "            \n",
    "            for instance in dev:\n",
    "                result = test_model.parser(instance)[0]\n",
    "                parsed_result.append(result)\n",
    "                \n",
    "            del test_model\n",
    "                \n",
    "            frameid, arg_precision, arg_recall, arg_f1, full_precision, full_recall, full_f1 = eval_fn.evaluate(dev, parsed_result)\n",
    "            d = {}\n",
    "            d['frameid'] = frameid\n",
    "            d['arg_precision'] = arg_precision\n",
    "            d['arg_recall'] = arg_recall\n",
    "            d['arg_f1'] = arg_f1\n",
    "            d['full_precision'] = full_precision\n",
    "            d['full_recall'] = full_recall\n",
    "            d['full_f1'] = full_f1\n",
    "            \n",
    "            if full_f1 > best_score:\n",
    "                model.save_pretrained(model_saved_path)\n",
    "                best_score = full_f1\n",
    "                \n",
    "                renew_stack = 0\n",
    "            else:\n",
    "                renew_stack +=1\n",
    "                \n",
    "            pprint(d)\n",
    "            print('Best score:', best_score)\n",
    "        \n",
    "            # 성능이 3epoch 이후에도 개선되지 않으면 중단\n",
    "            if renew_stack >= 3:\n",
    "                break\n",
    "            \n",
    "        elif early_stopping == False:\n",
    "            # save your model for each epochs\n",
    "            model_saved_path = model_dir+str(num_of_epoch)+'/'\n",
    "            if not os.path.exists(model_saved_path):\n",
    "                os.makedirs(model_saved_path)\n",
    "            model.save_pretrained(model_saved_path)\n",
    "\n",
    "            num_of_epoch += 1\n",
    "            \n",
    "        \n",
    "    print('...training is done. (', tac(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "your model would be saved at /disk/frameBERT/models/enModel-fn15/\n",
      "\n",
      "loading a pre-trained model...\n",
      "... is done. 0hour:0min:5sec\n",
      "\n",
      "converting data to BERT input...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... is done 0hour:0min:44sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../frameBERT/src/utils.py:290: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_logits = sm(masked_logit).view(1,-1)\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training is done. ( 0hour:0min:44sec )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = args.epochs\n",
    "# model_dir = '/disk/frameBERT/cltl_eval/models/efn_ekfn_jkfn_multitask'\n",
    "early_stopping = args.early_stopping\n",
    "batch_size = 6\n",
    "\n",
    "trn = trn_data[args.train]\n",
    "pre_trained_model = args.pretrained_model\n",
    "model_dir = args.model_path\n",
    "dev = tst_data[args.train]\n",
    "\n",
    "train(PRETRAINED_MODEL=pre_trained_model, \n",
    "      trn=trn, dev=dev,\n",
    "      epochs=epochs, model_dir=model_dir, fnversion=fnversion, \n",
    "      early_stopping=early_stopping, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
