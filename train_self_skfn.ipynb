{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###DEVICE: cuda\n",
      "\n",
      "###DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "import os\n",
    "from transformers import *\n",
    "from frameBERT.src import utils\n",
    "from frameBERT.src import dataio\n",
    "from frameBERT.src import eval_fn\n",
    "from frameBERT import frame_parser\n",
    "from frameBERT.src.modeling import BertForJointShallowSemanticParsing\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if device != \"cpu\":\n",
    "    torch.cuda.set_device(0)\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(0)   \n",
    "random.seed(0)\n",
    "\n",
    "from torch import autograd\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dir_path = os.path.dirname(os.path.abspath( __file__ ))\n",
    "except:\n",
    "    dir_path = '.'\n",
    "    \n",
    "# 실행시간 측정 함수\n",
    "import time\n",
    "\n",
    "_start_time = time.time()\n",
    "\n",
    "def tic():\n",
    "    global _start_time \n",
    "    _start_time = time.time()\n",
    "\n",
    "def tac():\n",
    "    t_sec = round(time.time() - _start_time)\n",
    "    (t_min, t_sec) = divmod(t_sec,60)\n",
    "    (t_hour,t_min) = divmod(t_min,60)\n",
    "    \n",
    "    result = '{}hour:{}min:{}sec'.format(t_hour,t_min,t_sec)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "\ttask: framenet\n",
      "\tlanguage: multilingual\n",
      "\tfn_version: 1.2\n",
      "used dictionary:\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_lu2idx.json\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_lufrmap.json\n",
      "\t /disk/frameBERT/frameBERT/src/../koreanframenet/resource/info/mul_bio_frargmap.json\n"
     ]
    }
   ],
   "source": [
    "srl = 'framenet'\n",
    "language = 'multilingual'\n",
    "fnversion = '1.2'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', required=False, help='모델 폴더', default='/disk/frameBERT/cltl_eval/models/efn_ekfn_multitask/34')\n",
    "parser.add_argument('--domain', required=True, help='도메인')\n",
    "parser.add_argument('--result', required=False, help='결과 저장 폴더', default=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('#####')\n",
    "print('\\ttask:', srl)\n",
    "print('\\tlanguage:', language)\n",
    "print('\\tfn_version:', fnversion)\n",
    "bert_io = utils.for_BERT(mode='train', language=language, masking=True, fnversion=fnversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of instances in trn: 19391\n",
      "# of instances in dev: 2272\n",
      "# of instances in tst: 6714\n",
      "data example: [['Greece', 'wildfires', 'force', 'thousands', 'to', '<tgt>', 'evacuate', '</tgt>'], ['_', '_', '_', '_', '_', '_', 'evacuate.v', '_'], ['_', '_', '_', '_', '_', '_', 'Escaping', '_'], ['O', 'O', 'O', 'B-Escapee', 'O', 'X', 'O', 'X']]\n",
      "\n",
      "### loading Korean FrameNet (from efn )\n",
      "tuples: (trn, tst)\n",
      "10647 3550\n",
      "\n",
      "### loading Korean FrameNet (from jfn )\n",
      "tuples: (trn, tst)\n",
      "2200 1000\n",
      "\n",
      "### loading Korean FrameNet (from sejong )\n",
      "tuples: (trn, unlabel_data, tst)\n",
      "500 4212 1000\n",
      "\n",
      "### loading Korean FrameNet (from propbank )\n",
      "tuples: (trn, unlabel_data, tst)\n",
      "500 852 1000\n"
     ]
    }
   ],
   "source": [
    "from koreanframenet import koreanframenet\n",
    "kfn = koreanframenet.interface(version=fnversion)\n",
    "\n",
    "en_trn, en_dev, en_tst = dataio.load_data(srl=srl, language='en')\n",
    "\n",
    "ekfn_trn_d, ekfn_tst_d = kfn.load_data(source='efn')\n",
    "jkfn_trn_d, jkfn_tst_d = kfn.load_data(source='jfn')\n",
    "skfn_trn_d, skfn_unlabel_d, skfn_tst_d = kfn.load_data(source='sejong')\n",
    "pkfn_trn_d, pkfn_unlabel_d, pkfn_tst_d = kfn.load_data(source='propbank')\n",
    "\n",
    "ekfn_trn = dataio.data2tgt_data(ekfn_trn_d, mode='train')\n",
    "ekfn_tst = dataio.data2tgt_data(ekfn_tst_d, mode='train')\n",
    "\n",
    "jkfn_trn = dataio.data2tgt_data(jkfn_trn_d, mode='train')\n",
    "jkfn_tst = dataio.data2tgt_data(jkfn_tst_d, mode='train')\n",
    "\n",
    "skfn_trn = dataio.data2tgt_data(skfn_trn_d, mode='train')\n",
    "skfn_unlabel = dataio.data2tgt_data(skfn_unlabel_d, mode='train')\n",
    "skfn_tst = dataio.data2tgt_data(skfn_tst_d, mode='train')\n",
    "\n",
    "pkfn_trn = dataio.data2tgt_data(pkfn_trn_d, mode='train')\n",
    "pkfn_unlabel = dataio.data2tgt_data(pkfn_unlabel_d, mode='train')\n",
    "pkfn_tst = dataio.data2tgt_data(pkfn_tst_d, mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = {}\n",
    "trn_data['ekfn'] = ekfn_trn\n",
    "trn_data['jkfn'] = jkfn_trn\n",
    "trn_data['skfn'] = skfn_trn\n",
    "trn_data['pkfn'] = pkfn_trn\n",
    "\n",
    "tst_data = {}\n",
    "tst_data['ekfn'] = ekfn_tst\n",
    "tst_data['jkfn'] = jkfn_tst\n",
    "tst_data['skfn'] = skfn_tst\n",
    "tst_data['pkfn'] = pkfn_tst\n",
    "\n",
    "unlabeled_data = {}\n",
    "unlabeled_data['ekfn'] = ekfn_trn\n",
    "unlabeled_data['jkfn'] = jkfn_trn\n",
    "unlabeled_data['skfn'] = skfn_trn + skfn_unlabel\n",
    "unlabeled_data['pkfn'] = pkfn_trn + pkfn_unlabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model: /disk/frameBERT/cltl_eval/models/efn_ekfn_finetune_pkfn/best\n"
     ]
    }
   ],
   "source": [
    "model_path = '/disk/frameBERT/cltl_eval/models/efn_ekfn_multitask/34'\n",
    "model_name = 'efn_ekfn_multitask/34'\n",
    "pretrained_model = args.model\n",
    "# pretrained_model = '/disk/frameBERT/models/enModel-fn17/2'\n",
    "print('pretrained_model:', pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Unlabeld data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_unlabeled_data(model_path, masking=True, language='ko', data='ekfn', threshold=0.7, added_list=[]):\n",
    "#     torch.cuda.set_device(device)\n",
    "    model = frame_parser.FrameParser(srl=srl,gold_pred=True, model_path=model_path, masking=masking, language=language, info=False)    \n",
    "    result = []\n",
    "    for i in range(len(unlabeled_data[data])):\n",
    "        instance = unlabeled_data[data][i]\n",
    "        \n",
    "        if i not in added_list:\n",
    "\n",
    "            parsed = model.parser(instance, result_format='all')        \n",
    "            conll = parsed['conll'][0]\n",
    "            frame_score = parsed['topk']['targets'][0]['frame_candidates'][0][-1]\n",
    "\n",
    "            if frame_score >= float(threshold):\n",
    "                parsed_result = conll\n",
    "                result.append(parsed_result)\n",
    "                added_list.append(i)\n",
    "            \n",
    "    added_list.sort()\n",
    "        \n",
    "    return result, added_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_path=\"bert-base-multilingual-cased\",\n",
    "          model_saved_path=False, epochs=10, batch_size=6, \n",
    "          trn=False): \n",
    "            \n",
    "    if not os.path.exists(model_saved_path):\n",
    "        os.makedirs(model_saved_path)\n",
    "    print('### START TRAINING:', model_saved_path)\n",
    "    # load a pre-trained model first\n",
    "    model = BertForJointShallowSemanticParsing.from_pretrained(model_path, \n",
    "                                                               num_senses = len(bert_io.sense2idx), \n",
    "                                                               num_args = len(bert_io.bio_arg2idx),\n",
    "                                                               lufrmap=bert_io.lufrmap, \n",
    "                                                               frargmap = bert_io.bio_frargmap)\n",
    "    model.to(device)\n",
    "    \n",
    "    print('\\nconverting data to BERT input...')\n",
    "    print('# of instances:', len(trn))\n",
    "    trn_data = bert_io.convert_to_bert_input_JointShallowSemanticParsing(trn)\n",
    "    sampler = RandomSampler(trn)\n",
    "    trn_dataloader = DataLoader(trn_data, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "    # load optimizer\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters()) \n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "    \n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        \n",
    "        # TRAIN loop\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            model.train()\n",
    "            # add batch to gpu\n",
    "            torch.cuda.set_device(device)\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_orig_tok_to_maps, b_input_lus, b_input_senses, b_input_args, b_token_type_ids, b_input_masks = batch            \n",
    "            loss = model(b_input_ids, lus=b_input_lus, senses=b_input_senses, args=b_input_args,\n",
    "                     token_type_ids=b_token_type_ids, attention_mask=b_input_masks)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()            \n",
    "\n",
    "    # save your model at 10 epochs\n",
    "    model.save_pretrained(model_saved_path)\n",
    "    print('... TRAINNG is DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### ITERATION: 1\n",
      "### PARSING START...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../frameBERT/src/utils.py:293: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_logits = sm(masked_logit).view(1,-1)\n",
      "../frameBERT/src/utils.py:301: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_logits = sm(masked_logit).view(1,-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... is done\n",
      "\n",
      "# of original training data: 500\n",
      "# of all unlabeled data: 852\n",
      "# of psuedo labeled data: 755 ((89, 2))\n",
      "Total Training Instance: 1255 \n",
      "\n",
      "[['증권전문가들은', '이틀', '연속', '대형우량주에', '매수세가', '형성되면서', '반등에', '<tgt>', '성공했으나', '</tgt>', '기관과', '개인투자자의', '매물공세가', '계속되는', '한', '불안정한', '흐름이', '이어질', '수밖에', '없다고', '내다봤다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '성공하다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Success_or_failure', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Agent', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[['레딩', '집행위원은', '유럽의', '학교간', '정보통신망', '구축을', '위해', '시험', '운용', '중인', '유럽학교통신망(EUN)도', '강화할', '필요가', '있다고', '밝혔다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '강화하다.v', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Cause_change_of_strength', '_', '_', '_'], ['B-Agent', 'I-Agent', 'O', 'O', 'O', 'O', 'O', 'B-Time', 'I-Time', 'I-Time', 'I-Time', 'O', 'O', 'O', 'O']]\n",
      "[['증권전문가들은', '이틀', '연속', '대형우량주에', '매수세가', '형성되면서', '반등에', '<tgt>', '성공했으나', '</tgt>', '기관과', '개인투자자의', '매물공세가', '계속되는', '한', '불안정한', '흐름이', '이어질', '수밖에', '없다고', '내다봤다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '성공하다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Success_or_failure', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Agent', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "### START TRAINING: /disk/frameBERT/cltl_eval/models/self_pkfn_using_efn_ekfn_finetuned_pkfn/1/\n",
      "\n",
      "converting data to BERT input...\n",
      "# of instances: 1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:  10%|█         | 1/10 [01:20<12:08, 80.90s/it]\u001b[A\n",
      "Epoch:  20%|██        | 2/10 [02:44<10:53, 81.72s/it]\u001b[A\n",
      "Epoch:  30%|███       | 3/10 [04:13<09:46, 83.85s/it]\u001b[A\n",
      "Epoch:  40%|████      | 4/10 [05:42<08:33, 85.50s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 5/10 [07:12<07:13, 86.74s/it]\u001b[A\n",
      "Epoch:  60%|██████    | 6/10 [08:41<05:50, 87.60s/it]\u001b[A\n",
      "Epoch:  70%|███████   | 7/10 [10:11<04:24, 88.24s/it]\u001b[A\n",
      "Epoch:  80%|████████  | 8/10 [11:41<02:57, 88.70s/it]\u001b[A\n",
      "Epoch:  90%|█████████ | 9/10 [13:11<01:29, 89.06s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 10/10 [14:41<00:00, 88.14s/it]\u001b[A\n",
      "Iteration:  10%|█         | 1/10 [15:57<2:23:41, 957.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TRAINNG is DONE\n",
      "\n",
      "### ITERATION: 2\n",
      "### PARSING START...\n",
      "... is done\n",
      "\n",
      "# of original training data: 500\n",
      "# of all unlabeled data: 852\n",
      "# of psuedo labeled data: 802 ((94, 2))\n",
      "Total Training Instance: 1302 \n",
      "\n",
      "[['증권전문가들은', '이틀', '연속', '대형우량주에', '매수세가', '형성되면서', '반등에', '<tgt>', '성공했으나', '</tgt>', '기관과', '개인투자자의', '매물공세가', '계속되는', '한', '불안정한', '흐름이', '이어질', '수밖에', '없다고', '내다봤다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '성공하다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Success_or_failure', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Agent', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[['레딩', '집행위원은', '유럽의', '학교간', '정보통신망', '구축을', '위해', '시험', '운용', '중인', '유럽학교통신망(EUN)도', '강화할', '필요가', '있다고', '밝혔다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '강화하다.v', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Cause_change_of_strength', '_', '_', '_'], ['B-Agent', 'I-Agent', 'O', 'O', 'O', 'O', 'O', 'B-Time', 'I-Time', 'I-Time', 'I-Time', 'O', 'O', 'O', 'O']]\n",
      "[['증권전문가들은', '이틀', '연속', '대형우량주에', '매수세가', '형성되면서', '반등에', '<tgt>', '성공했으나', '</tgt>', '기관과', '개인투자자의', '매물공세가', '계속되는', '한', '불안정한', '흐름이', '이어질', '수밖에', '없다고', '내다봤다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '성공하다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Success_or_failure', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Agent', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "### START TRAINING: /disk/frameBERT/cltl_eval/models/self_pkfn_using_efn_ekfn_finetuned_pkfn/2/\n",
      "\n",
      "converting data to BERT input...\n",
      "# of instances: 1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:  10%|█         | 1/10 [01:30<13:37, 90.86s/it]\u001b[A\n",
      "Epoch:  20%|██        | 2/10 [03:04<12:12, 91.55s/it]\u001b[A\n",
      "Epoch:  30%|███       | 3/10 [04:37<10:44, 92.03s/it]\u001b[A\n",
      "Epoch:  40%|████      | 4/10 [06:10<09:14, 92.36s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 5/10 [07:43<07:43, 92.62s/it]\u001b[A\n",
      "Epoch:  60%|██████    | 6/10 [09:16<06:11, 92.81s/it]\u001b[A\n",
      "Epoch:  70%|███████   | 7/10 [10:50<04:38, 92.96s/it]\u001b[A\n",
      "Epoch:  80%|████████  | 8/10 [12:23<03:06, 93.03s/it]\u001b[A\n",
      "Epoch:  90%|█████████ | 9/10 [13:56<01:33, 93.11s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 10/10 [15:29<00:00, 92.98s/it]\u001b[A\n",
      "Iteration:  20%|██        | 2/10 [31:44<2:07:16, 954.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... TRAINNG is DONE\n",
      "\n",
      "### ITERATION: 3\n",
      "### PARSING START...\n",
      "... is done\n",
      "\n",
      "# of original training data: 500\n",
      "# of all unlabeled data: 852\n",
      "# of psuedo labeled data: 824 ((97, 2))\n",
      "Total Training Instance: 1324 \n",
      "\n",
      "[['증권전문가들은', '이틀', '연속', '대형우량주에', '매수세가', '형성되면서', '반등에', '<tgt>', '성공했으나', '</tgt>', '기관과', '개인투자자의', '매물공세가', '계속되는', '한', '불안정한', '흐름이', '이어질', '수밖에', '없다고', '내다봤다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '성공하다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Success_or_failure', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Agent', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "[['레딩', '집행위원은', '유럽의', '학교간', '정보통신망', '구축을', '위해', '시험', '운용', '중인', '유럽학교통신망(EUN)도', '강화할', '필요가', '있다고', '밝혔다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '강화하다.v', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'Cause_change_of_strength', '_', '_', '_'], ['B-Agent', 'I-Agent', 'O', 'O', 'O', 'O', 'O', 'B-Time', 'I-Time', 'I-Time', 'I-Time', 'O', 'O', 'O', 'O']]\n",
      "[['증권전문가들은', '이틀', '연속', '대형우량주에', '매수세가', '형성되면서', '반등에', '<tgt>', '성공했으나', '</tgt>', '기관과', '개인투자자의', '매물공세가', '계속되는', '한', '불안정한', '흐름이', '이어질', '수밖에', '없다고', '내다봤다.'], ['_', '_', '_', '_', '_', '_', '_', '_', '성공하다.v', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'Success_or_failure', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['B-Agent', 'O', 'O', 'O', 'O', 'O', 'O', 'X', 'O', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "### START TRAINING: /disk/frameBERT/cltl_eval/models/self_pkfn_using_efn_ekfn_finetuned_pkfn/3/\n",
      "\n",
      "converting data to BERT input...\n",
      "# of instances: 1324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:  10%|█         | 1/10 [01:33<13:57, 93.03s/it]\u001b[A\n",
      "Epoch:  20%|██        | 2/10 [03:08<12:28, 93.62s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "model_saved_dir = '/disk/frameBERT/cltl_eval/models/'\n",
    "\n",
    "if args.result:\n",
    "    result_dir = args.result\n",
    "else:\n",
    "    result_dir = 'self_'+ args.domain +'_using_'+ args.model\n",
    "model_saved_dir = model_saved_dir + result_dir\n",
    "\n",
    "if model_saved_dir[-1] != '/':\n",
    "    model_saved_dir = model_saved_dir+'/'\n",
    "    \n",
    "if not os.path.exists(model_saved_dir):\n",
    "    os.makedirs(model_saved_dir)\n",
    "print('your models are saved to', model_saved_path)\n",
    "    \n",
    "iters = 5\n",
    "threshold = 0.9\n",
    "instance = []\n",
    "added_list = []\n",
    "batch_size = 6\n",
    "\n",
    "for _ in trange(iters, desc=\"Iteration\"):\n",
    "    iteration = _ + 1    \n",
    "    \n",
    "    if iteration == 1:\n",
    "        pre_model = BertForJointShallowSemanticParsing.from_pretrained(pretrained_model, \n",
    "                                                               num_senses = len(bert_io.sense2idx), \n",
    "                                                               num_args = len(bert_io.bio_arg2idx),\n",
    "                                                               lufrmap=bert_io.lufrmap, \n",
    "                                                               frargmap = bert_io.bio_frargmap)\n",
    "        pre_model.to(device)\n",
    "        \n",
    "        model_saved_path = model_saved_dir+'0/'\n",
    "        if not os.path.exists(model_saved_path):\n",
    "            os.makedirs(model_saved_path)\n",
    "        pre_model.save_pretrained(model_saved_path)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    parsing_model_path = model_saved_dir + str(iteration-1) +'/'\n",
    "    model_saved_path = model_saved_dir+str(iteration)+'/'\n",
    "    if not os.path.exists(model_saved_path):\n",
    "        os.makedirs(model_saved_path)\n",
    "    \n",
    "    print('\\n### ITERATION:', str(iteration))\n",
    "    trn = trn_data['ekfn']\n",
    "    print('### PARSING START...')\n",
    "    parsed_result, added_list = parsing_unlabeled_data(parsing_model_path, data=args.domain, \n",
    "                                                       masking=True, \n",
    "                                                       threshold=threshold, added_list=added_list)\n",
    "    instance += parsed_result\n",
    "    print('... is done')\n",
    "    \n",
    "    # training process\n",
    "    trn_instance = trn + instance\n",
    "    \n",
    "    print('\\n# of original training data:', len(trn))\n",
    "    print('# of all unlabeled data:', len(unlabeled_data[args.domain]))\n",
    "    print('# of psuedo labeled data:', len(instance), '('+str((round(len(instance)/len(unlabeled_data[args.domain])*100), 2))+'%)')\n",
    "    print('Total Training Instance:', len(trn_instance), '\\n') \n",
    "    \n",
    "    train(model_path=parsing_model_path, model_saved_path=model_saved_path, trn=trn_instance)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
